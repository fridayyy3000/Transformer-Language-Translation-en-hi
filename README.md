# ğŸ” Neural Machine Translation using Transformers (English â†” Hindi)

This project implements a Transformer-based Neural Machine Translation (NMT) model for translating between English and Hindi, inspired by the landmark paper:  
**[Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)**.

## ğŸš€ Overview

- âœ… Built from scratch in PyTorch  
- âœ… Custom tokenizer using HuggingFace `tokenizers`  
- âœ… Dataset: [cfilt/iitb-english-hindi](https://huggingface.co/datasets/cfilt/iitb-english-hindi)  
- âœ… Implements multi-head attention, positional encoding, and encoder-decoder structure  
- âœ… Trained on 90K+ sentence pairs with manual + interval-based checkpoint saving  
- âœ… Translates live from English â¡ Hindi using a CLI

---

## ğŸ§  Architecture

- Based on the **original Transformer encoder-decoder**:
  - **Encoder**: Self-attention + Feedforward layers
  - **Decoder**: Masked self-attention + Encoder attention
  - **Positional Encoding** to inject sequence information
- Implemented with modular, readable `model.py` for clarity and reusability
---

## ğŸ› ï¸ How to Use

### 1. Install Requirements
torch>=2.0.0
tokenizers>=0.13.3
datasets>=2.18.0
tqdm>=4.66.0
tensorboard>=2.15.0

pip install -r requirements.txt

Optional (if you're using GPU on Mac)
If you're using Apple M1/M2 GPU acceleration (MPS):

# Optional - Apple Silicon optimized build (PyTorch)
# Uncomment if using Apple M1/M2
# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/mps

Input Sentence (English) â”€â–º Tokenizer â”€â–º Positional Encoding â”€â–º Encoder â”€â”
â–¼
Decoder â—„â”€â”€â”€â”€ Masked Attention
â—„â”€â”€â”€â”€ Target Tokens (Hindi)
â–¼
Linear + Softmax â”€â–º Prediction


- **Encoder**: Multi-head self-attention + feedforward
- **Decoder**: Masked self-attention, encoder-decoder attention, and feedforward
- **Loss**: CrossEntropy with label smoothing and `[PAD]` masking

## ğŸ§ª Sample Results

| English Input             | Hindi Translation           |
|--------------------------|-----------------------------|
| What is your name?       | à¤†à¤ªà¤•à¤¾ à¤¨à¤¾à¤® à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ ?         |
| My name is Gaurav        | à¤®à¥‡à¤°à¤¾ à¤¨à¤¾à¤® à¤—à¥Œà¤°à¤µ à¤¹à¥ˆ           |
| Today is a good day      | à¤†à¤œ à¤à¤• à¤…à¤šà¥à¤›à¤¾ à¤¦à¤¿à¤¨ à¤¹à¥ˆ        |
| Hello                    | à¤¨à¤®à¤¸à¥à¤¤à¥‡                      |


ğŸ”§ Configuration
Defined in config.py:

python
{
  "batch_size": 16,
  "num_epochs": 1,
  "lr": 5e-4,
  "seq_len": 128,
  "d_model": 256,
  "datasource": "cfilt/iitb-english-hindi",
  "lang_src": "en",
  "lang_tgt": "hi"
}

ğŸ“ˆ Training
To train the model from scratch:

python3 train.py
Tokenizers are built and cached

Model weights are saved at 10% intervals and once at the end of training

TensorBoard logs are written to runs/

ğŸ“¤ Inference
Once training is complete:

python3 translate.py

ğŸ“ Project Structure
â”œâ”€â”€ config.py              # All hyperparameters and paths
â”œâ”€â”€ train.py               # Training script
â”œâ”€â”€ translate.py           # Inference script
â”œâ”€â”€ model.py               # Transformer architecture
â”œâ”€â”€ dataset.py             # Data pipeline and collators
â”œâ”€â”€ weights/               # Saved model checkpoints
â”œâ”€â”€ runs/                  # TensorBoard logs
â””â”€â”€ tokenizer_en.json / tokenizer_hi.json
ğŸ§  Citation & Reference
Vaswani et al. (2017): Attention Is All You Need
https://arxiv.org/abs/1706.03762
